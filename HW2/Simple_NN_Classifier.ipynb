{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6MBVRSro5tt",
        "outputId": "c8d0ba58-d64d-4d0b-df50-4a62ef4c8237"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"movie_reviews\")\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import random\n",
        "\n",
        "random.seed(0) # Don't change\n",
        "torch.manual_seed(0)  # Don't change\n",
        "np.random.seed(0) # Don't change\n",
        "\n",
        "\n",
        "train_X, train_Y = [], []\n",
        "test_X, test_Y = [], []\n",
        "\n",
        "for polarity in movie_reviews.categories():\n",
        "    label = 0 if polarity == 'neg' else 1\n",
        "    for fid in movie_reviews.fileids(polarity):\n",
        "        if random.randrange(5) == 0:\n",
        "            test_X.append([w for w in movie_reviews.words(fid)])\n",
        "            test_Y.append(label)\n",
        "        else:\n",
        "            train_X.append([w for w in movie_reviews.words(fid)])\n",
        "            train_Y.append(label)\n",
        "\n",
        "print(train_X[0], train_Y[0])"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'] 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment II\n",
        "Doing Assignment II by modifying the following code cell.\n",
        "Your solution should be based on feedforward neural network (FNN or MLP) with word embeddings.\n",
        "You are free to adjust the FNN with different dimension settings, vocabulary, overfitting prevention, and so on,\n",
        "but you can not use other architectures (e.g., CNN/RNN/Transformer or the Naive Bayes classifier from Assignment I) in this assignment.\n"
      ],
      "metadata": {
        "id": "ds984vYgcX7V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ru6_YNo_Z-"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "EMBEDDING_DIM = 150\n",
        "EPOCHS = 10\n",
        "\n",
        "out_n1 = 65\n",
        "out_n2 = 20\n",
        "out_n3 = 15\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  def init_embeddings(self, vocab):\n",
        "    self.word_to_ix = {}\n",
        "    weights = []\n",
        "    ix = 0\n",
        "    for w in vocab:\n",
        "      self.word_to_ix[w] = ix\n",
        "      ix += 1\n",
        "    self.vocab_size = len(self.word_to_ix)\n",
        "    self.embeddings = nn.EmbeddingBag(self.vocab_size, EMBEDDING_DIM)\n",
        "\n",
        "  def __init__(self, vocab, classes):\n",
        "    super(TextClassifier, self).__init__()\n",
        "    self.classes = classes\n",
        "    self.init_embeddings(vocab)\n",
        "    self.fc1 = nn.Linear(self.embeddings.embedding_dim, out_n1)\n",
        "    self.fc1.weight.data.uniform_(-0.5, 0.5)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2 = nn.Linear(out_n1, out_n2)\n",
        "    self.fc2.weight.data.uniform_(-0.5, 0.5)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3 = nn.Linear(out_n2, out_n3)\n",
        "    self.fc3.weight.data.uniform_(-0.5, 0.5)\n",
        "    self.fc3.bias.data.zero_()\n",
        "    self.out = nn.Linear(out_n3, len(self.classes))\n",
        "    self.out.weight.data.uniform_(-0.5, 0.5)\n",
        "    self.out.bias.data.zero_()\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, inputs, offsets):\n",
        "    embedded = self.embeddings(inputs, offsets)\n",
        "    return self.out(self.relu(self.fc3(self.relu(self.fc2(self.relu(self.fc1(embedded)))))))\n",
        "\n",
        "def make_doc_vector(doc, word_to_ix):\n",
        "  idxs = [word_to_ix[w] for w in doc if w in word_to_ix]\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "def generate_batch(batch):\n",
        "  label = torch.tensor([entry[0] for entry in batch])\n",
        "  text = [entry[1] for entry in batch]\n",
        "  offsets = [0] + [len(entry) for entry in text]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text = torch.cat(text)\n",
        "  return text, offsets, label\n",
        "\n",
        "def build_vocab(X):\n",
        "  word_count = Counter()\n",
        "  for x in X:\n",
        "    for w in x:\n",
        "      word_count[w] += 1\n",
        "  # The order of keys in a dictionary/set is not deterministic,\n",
        "  # so sorting in the following statement is important to avoid randomness.\n",
        "  return [w for (w, c) in sorted(word_count.items()) if c >= 16]\n",
        "\n",
        "def build_model(X, Y):\n",
        "  model = TextClassifier(build_vocab(X), [0, 1]).to(device)\n",
        "  loss_function = nn.CrossEntropyLoss().to(device)\n",
        "  optimizer = optim.RMSprop(model.parameters())\n",
        "\n",
        "  train_set = []\n",
        "  yc = Counter()\n",
        "  for x, y in zip(X, Y):\n",
        "    entry = []\n",
        "    yc[y] += 1\n",
        "    entry.append(torch.LongTensor([y]))\n",
        "    entry.append(make_doc_vector(x, model.word_to_ix))\n",
        "    train_set.append(entry)\n",
        "  print(yc)\n",
        "  data = DataLoader(train_set, batch_size=36, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    print(\"Epoch: %d\" % epoch)\n",
        "    for _, (x, offsets, y) in enumerate(data):\n",
        "      model.zero_grad()\n",
        "      x, offsets, y = x.to(device), offsets.to(device), y.to(device)\n",
        "      pred = model(x, offsets)\n",
        "      loss = loss_function(pred, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "      train_acc += (pred.argmax(1) == y).sum().item()\n",
        "    print(\"Loss: %g, Acc: %g\" % (train_loss / len(train_set), train_acc / len(train_set)))\n",
        "  return model"
      ],
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aek5tb3yulJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c27c34-0dae-4911-bcfd-bb9901aa774c"
      },
      "source": [
        "model = build_model(train_X, train_Y)"
      ],
      "execution_count": 423,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 803, 0: 775})\n",
            "Epoch: 0\n",
            "Loss: 0.0229784, Acc: 0.579214\n",
            "Epoch: 1\n",
            "Loss: 0.013443, Acc: 0.801014\n",
            "Epoch: 2\n",
            "Loss: 0.00559276, Acc: 0.923321\n",
            "Epoch: 3\n",
            "Loss: 0.00266969, Acc: 0.966413\n",
            "Epoch: 4\n",
            "Loss: 0.00036812, Acc: 0.996198\n",
            "Epoch: 5\n",
            "Loss: 6.46709e-05, Acc: 0.999366\n",
            "Epoch: 6\n",
            "Loss: 1.79675e-05, Acc: 1\n",
            "Epoch: 7\n",
            "Loss: 6.26188e-06, Acc: 1\n",
            "Epoch: 8\n",
            "Loss: 3.50013e-06, Acc: 1\n",
            "Epoch: 9\n",
            "Loss: 2.09503e-06, Acc: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT85z26VAInH",
        "outputId": "a8fa04a2-58cd-4819-fb7d-0ee998d3e6e4"
      },
      "source": [
        "def predict(model, document):\n",
        "  probs = model(make_doc_vector(document, model.word_to_ix).to(device), torch.tensor([0]).cumsum(dim=0).to(device))\n",
        "  return int(torch.argmax(probs))\n",
        "\n",
        "print(predict(model, \"this is a uninteresting movie\".split(\" \")))\n",
        "print(predict(model, \"a good movie of this year\".split(\" \")))\n"
      ],
      "execution_count": 424,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo1u7CAk6bvR"
      },
      "source": [
        "## Do Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct, total = 0, 0\n",
        "\n",
        "for x, y in zip(test_X, test_Y):\n",
        "    prediction = predict(model, x)\n",
        "    if prediction == y:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "print(\"%d / %d = %g\" % (correct, total, correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eph3yR9Ma3rP",
        "outputId": "f119349b-a7bc-41c7-c886-d1032480e597"
      },
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "366 / 422 = 0.867299\n"
          ]
        }
      ]
    }
  ]
}
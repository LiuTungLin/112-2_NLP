# -*- coding: utf-8 -*-
"""[NLP Assignment III] Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JPmnDK1bxVNIYOesoXyck9qsMwHWwOA3
"""

!pip install transformers==4.40.2 datasets evaluate accelerate simpletransformers

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader

import nltk
from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize
nltk.download("movie_reviews")

from collections import defaultdict, Counter
import math
import random

random.seed(0) # Don't change
torch.manual_seed(0)  # Don't change
np.random.seed(0) # Don't change

train_X, train_Y = [], []
test_X, test_Y = [], []

for polarity in movie_reviews.categories():
    label = 0 if polarity == 'neg' else 1
    for fid in movie_reviews.fileids(polarity):
        if random.randrange(5) == 0:
            test_X.append(movie_reviews.raw(fid))
            test_Y.append(label)
        else:
            train_X.append(movie_reviews.raw(fid))
            train_Y.append(label)
print(train_X[0], train_Y[0])

"""# Assignment III
Doing Assignment III by modifying the following code cell.
Your goal is to train a Transformer based model to achieve an Accuracy of 0.85. The higher, the better.

Your solution should be based on Transformer models.
You are free to adjust [the settings](https://simpletransformers.ai/docs/usage/) of the simpletransformers class and [the pre-trained models](https://huggingface.co/models?sort=trending). You can also build the model with [the transformers package](https://huggingface.co/docs/transformers/en/index) for more control in details, but your model has to follow the output format of [the predict() method](https://simpletransformers.ai/docs/classification-models/) of `simpletransformers` for automatic evaluation.

Please note that your code is expected to be executed in the free version of Colab. So please make sure your model and your training procedure is lightweight enough to be successfully executed by our TAs.
"""

from simpletransformers.classification import ClassificationModel
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

def build_model(training_instances, training_labels):
  train_data = zip(training_instances, training_labels)
  train_df = pd.DataFrame(train_data)
  # use_cuda=True if GPU is available
  model = ClassificationModel("bert", "bert-base-uncased", use_cuda=True, args={
      'overwrite_output_dir': True, 'num_train_epochs': 2})
  model.train_model(train_df)
  return model

model = build_model(train_X, train_Y)

def predict(model, document):
    return model.predict([document])[0][0]

print(predict(model, "this is an uninteresting movie"))
print(predict(model, "a good movie of this year"))

"""## Do Evaluation"""

correct, total = 0, 0

pred_Y = model.predict(test_X)[0]

for prediction, y in zip(pred_Y, test_Y):
    if prediction == y:
        correct += 1
    total += 1

print("%d / %d = %g" % (correct, total, correct / total))

